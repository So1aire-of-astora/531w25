---
title: "Gruver, Finzi, Qiu & Wilson (2023)"
date: "Feb 20, 2025"
output:
  ioslides_presentation:
    smaller: no
    widescreen: true
    transition: "faster" 
---

## Impact

* cited 337 times

* NeurIPS


## Insights

* Fig 2 is impressive

* Tokenization ic critical

* Ability of LLMs to represent complex distributions

* Generalizes from all known time series, as if it has learned a huge random effects model.

## Subsequent work

[Time-LLM: Time series forecasting by reprogramming large language models](https://arxiv.org/pdf/2310.01728)

A much more complicated embedding mapping into text and back out again

## Computers can out-guess humans

* Can an LLM predict the stock market?

* Back to the question of forecasting vs other time series purposes (fitting a model to answer scientific questions).

